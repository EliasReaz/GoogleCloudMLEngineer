## ðŸ“‘ Summary: *Design and Build an Input Data Pipeline (Google Cloud ML Engineer, Course2/Module 2)*

### 1. ML Workflow Recap
- ML projects move from **data extraction â†’ analysis â†’ preparation â†’ training â†’ evaluation â†’ validation â†’ serving â†’ monitoring**.
- Two phases: **training** (labeled data â†’ model) and **inference** (new data â†’ predictions).
- Features must be represented as **real-numbered vectors**; better features = faster training + more accurate predictions.

---

### 2. Importance of Efficient Data Pipelines
- Training steps involve:
  1. Opening files
  2. Fetching entries
  3. Using data for training
- Efficient pipelines reduce overhead and keep GPUs busy.

---

### 3. TensorFlow `tf.data` API
- Builds **complex pipelines from reusable pieces**.
- Handles **large datasets, multiple formats, and transformations**.
- Examples:
  - Image pipeline: load files, apply random perturbations, batch.
  - Text pipeline: tokenize, map to embeddings, batch sequences.

---

### 4. Dataset Abstractions
- `tf.data.Dataset` represents a sequence of elements (e.g., image + label).
- Two creation modes:
  - **Data source**: from memory or files.
  - **Data transformation**: from existing datasets.

---

### 5. Handling Large Datasets
- Large datasets are **sharded** into multiple files (e.g., `train.csv-00000-of-00011`).
- Training uses **mini-batches**, not full dataset in memory.
- Specialized dataset classes:
  - `TextLineDataset` â†’ text/CSV
  - `TFRecordDataset` â†’ TFRecord files
  - `FixedLengthRecordDataset` â†’ binary fixed-length records

---

### 6. Examples
- **TFRecordDataset**: chain `shuffle â†’ map â†’ batch â†’ iterator`.
- **In-memory tensors**: `from_tensors()` vs `from_tensor_slices()`.
- **CSV files**: `TextLineDataset` + `map(parse_row)` + shuffle/batch/repeat.
- **Sharded CSVs**: `list_files()` + `flat_map(TextLineDataset)` + `map(parse_row)`.

---

### 7. Performance Optimizations
- **Prefetching**: CPU prepares next batch while GPU trains current batch.
- **Multithreading**: parallel preprocessing keeps GPUs fully utilized.

---

### 8. Feature Columns
- Define model inputs:
  - `numeric_column` (e.g., square footage)
  - `categorical_column_with_vocabulary_list` (e.g., house vs apartment)
- Transformations:
  - **One-hot encoding**
  - **Bucketized columns** (numeric ranges)
  - **Embedding columns** (dense, lower-dimensional vectors for large categories)
  - **Crossed columns** (feature interactions)
- Feature columns pack raw inputs into model-ready vectors.

---

### 9. Embeddings
- Purpose:
  1. Nearest neighbors (recommendations, clustering)
  2. Input to supervised ML tasks
  3. Visualization of categories/concepts
- Examples:
  - MNIST digit clustering in TensorBoard
  - Movie recommendation systems (users Ã— movies matrix â†’ embeddings)

---

## ðŸŽ¯ Key Takeaways
- Efficient pipelines are critical for scaling ML training.
- `tf.data` API provides modular, reusable building blocks.
- Feature columns and embeddings transform raw data into model-ready vectors.
- Prefetching + multithreading maximize GPU utilization.
- Embeddings enable handling of large categorical spaces and power recommendation systems.

---
